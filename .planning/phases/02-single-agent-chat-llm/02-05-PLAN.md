---
phase: 02-single-agent-chat-llm
plan: 05
type: execute
wave: 3
depends_on: ["02-01", "02-03", "02-04"]
files_modified:
  - crates/boternity-core/src/agent/mod.rs
  - crates/boternity-core/src/agent/engine.rs
  - crates/boternity-core/src/agent/context.rs
  - crates/boternity-core/src/agent/prompt.rs
  - crates/boternity-core/src/chat/service.rs
  - crates/boternity-core/src/chat/session.rs
  - crates/boternity-core/src/chat/mod.rs
  - crates/boternity-core/src/lib.rs
autonomous: true

must_haves:
  truths:
    - "System prompt is composed from SOUL.md + IDENTITY.md config + USER.md + session memories with XML tag boundaries"
    - "Agent execution loop sends user message through LLM and returns a stream of events"
    - "ChatService manages session lifecycle: create, send_message, end_session"
    - "Token budget prevents context window overflow by tracking usage"
    - "Bot personality files (soul, identity, user) are read and injected into the system prompt"
  artifacts:
    - path: "crates/boternity-core/src/agent/engine.rs"
      provides: "AgentEngine with execute() method returning StreamEvent stream"
      contains: "pub struct AgentEngine"
    - path: "crates/boternity-core/src/agent/prompt.rs"
      provides: "SystemPromptBuilder assembling soul + identity + user + memories"
      contains: "SystemPromptBuilder"
    - path: "crates/boternity-core/src/agent/context.rs"
      provides: "AgentContext holding conversation state, soul, and memories"
      contains: "pub struct AgentContext"
    - path: "crates/boternity-core/src/chat/service.rs"
      provides: "ChatService orchestrating sessions, messages, and agent calls"
      contains: "pub struct ChatService"
  key_links:
    - from: "crates/boternity-core/src/agent/engine.rs"
      to: "crates/boternity-core/src/llm/box_provider.rs"
      via: "AgentEngine holds BoxLlmProvider for runtime dispatch"
      pattern: "BoxLlmProvider"
    - from: "crates/boternity-core/src/agent/prompt.rs"
      to: "crates/boternity-types/src/memory.rs"
      via: "injects MemoryEntry facts into system prompt"
      pattern: "MemoryEntry|session_memory"
    - from: "crates/boternity-core/src/chat/service.rs"
      to: "crates/boternity-core/src/agent/engine.rs"
      via: "ChatService delegates to AgentEngine for LLM calls"
      pattern: "AgentEngine|engine"
---

<objective>
Build the agent execution engine and chat service that orchestrate bot conversations.

Purpose: This is the brain of the chat system. The agent engine assembles the system prompt from the bot's personality files and memories, sends messages through the LLM provider, and returns streaming responses. The chat service manages the session lifecycle (create, message, end) and coordinates persistence. Together they implement the core loop: user types -> agent processes -> LLM streams -> response delivered.

Output: AgentEngine, SystemPromptBuilder, AgentContext, ChatService, and SessionManager in boternity-core.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-single-agent-chat-llm/02-RESEARCH.md
@.planning/phases/02-single-agent-chat-llm/02-CONTEXT.md
@.planning/phases/02-single-agent-chat-llm/02-01-SUMMARY.md
@.planning/phases/02-single-agent-chat-llm/02-03-SUMMARY.md
@.planning/phases/02-single-agent-chat-llm/02-04-SUMMARY.md
@crates/boternity-core/src/service/bot.rs
@crates/boternity-core/src/service/soul.rs
@crates/boternity-core/src/service/fs.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SystemPromptBuilder and AgentContext</name>
  <files>
    crates/boternity-core/src/agent/mod.rs
    crates/boternity-core/src/agent/context.rs
    crates/boternity-core/src/agent/prompt.rs
    crates/boternity-core/src/lib.rs
  </files>
  <action>
**context.rs** -- AgentContext holds all state needed for a conversation:
- `pub struct AgentContext` with fields:
  - `agent_config: AgentConfig` -- bot identity (name, emoji, model, etc.)
  - `soul_content: String` -- SOUL.md raw content
  - `identity_content: String` -- IDENTITY.md raw content
  - `user_content: String` -- USER.md raw content
  - `memories: Vec<MemoryEntry>` -- session memories loaded at start
  - `conversation_history: Vec<Message>` -- accumulated messages in this session
  - `token_budget: TokenBudget` -- context window allocation
  - `system_prompt: String` -- the assembled system prompt (built once, updated when memories change)
- Methods:
  - `pub fn new(config: AgentConfig, soul: String, identity: String, user: String, memories: Vec<MemoryEntry>, token_budget: TokenBudget) -> Self`
  - `pub fn add_user_message(&mut self, content: String)` -- push Message { role: User, content }
  - `pub fn add_assistant_message(&mut self, content: String)` -- push Message { role: Assistant, content }
  - `pub fn build_messages(&self) -> Vec<Message>` -- return conversation_history (system prompt goes separately)
  - `pub fn should_summarize(&self) -> bool` -- delegate to token_budget

**prompt.rs** -- SystemPromptBuilder:
- Use the template from RESEARCH.md Pattern 4. XML tags for section boundaries:
  ```
  <soul>{soul_content}</soul>
  <identity>Name: {name}\nEmoji: {emoji}\nModel: {model}</identity>
  <user_context>{user_md_content}</user_context>
  <session_memory>Key points from previous conversations:\n{memory entries}</session_memory>
  <instructions>You are {name}. Always stay in character... Express personality strongly... Reference past conversations naturally without "I remember"... When uncertain, acknowledge it.</instructions>
  ```
- `pub struct SystemPromptBuilder`
- `pub fn build(config: &AgentConfig, soul: &str, identity: &str, user: &str, memories: &[MemoryEntry]) -> String`
  - Format each memory as "- {fact} [{category}]" (one per line)
  - If no memories, omit the <session_memory> section entirely
  - If user content is empty/default, omit <user_context> section
  - Always include <soul>, <identity>, and <instructions>
  - Per locked decision: "Strong personality" + "Express personality strongly in every response"
  - Per locked decision: "Reference past conversations naturally without saying 'I remember...'"

**mod.rs:** `pub mod engine; pub mod context; pub mod prompt;`

Update `crates/boternity-core/src/lib.rs` to add `pub mod agent;`
  </action>
  <verify>
Run `cargo check -p boternity-core` -- AgentContext and SystemPromptBuilder compile. Verify the system prompt template includes XML tags for all sections.
  </verify>
  <done>AgentContext holds conversation state with token budget awareness. SystemPromptBuilder assembles soul + identity + user + memories into an XML-tagged system prompt following the locked personality decisions.</done>
</task>

<task type="auto">
  <name>Task 2: Create AgentEngine and ChatService</name>
  <files>
    crates/boternity-core/src/agent/engine.rs
    crates/boternity-core/src/chat/service.rs
    crates/boternity-core/src/chat/session.rs
    crates/boternity-core/src/chat/mod.rs
  </files>
  <action>
**engine.rs** -- AgentEngine (the core execution loop):
- `pub struct AgentEngine` with field: provider (BoxLlmProvider)
- `impl AgentEngine`:
  - `pub fn new(provider: BoxLlmProvider) -> Self`
  - `pub fn execute(&self, context: &AgentContext, user_message: &str) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, LlmError>> + Send + 'static>>`
    - Build CompletionRequest from context:
      - model: context.agent_config.model
      - system: Some(context.system_prompt.clone())
      - messages: context.build_messages() + the new user message
      - max_tokens: context.agent_config.max_tokens
      - temperature: Some(context.agent_config.temperature)
      - stream: true
    - Call provider.stream(request) -- delegate to BoxLlmProvider
    - Return the stream directly (Phase 2 has no tools, so no intermediate iterations needed)
  - `pub async fn execute_non_streaming(&self, context: &AgentContext, user_message: &str) -> Result<CompletionResponse, LlmError>`
    - Same request building but with stream: false
    - Call provider.complete(&request)
    - Used for memory extraction, title generation, and context summarization (internal LLM calls that don't stream to CLI)
  - `pub async fn generate_greeting(&self, context: &AgentContext) -> Result<String, LlmError>`
    - Per locked decision: "Bot speaks first -- sends a personality-driven greeting message when session opens"
    - Build a one-shot request with system prompt + a single user message: "Generate a short greeting message in character. Be warm and inviting. Keep it to 1-2 sentences."
    - Call execute_non_streaming, return the response content

Add `use tracing::info_span;` and instrument the execute methods with OTel GenAI spans:
- Span name: `"{operation} {model}"` per convention
- Attributes: gen_ai.operation.name, gen_ai.provider.name, gen_ai.request.model, gen_ai.request.temperature, gen_ai.request.max_tokens
- Record usage attributes after completion (for non-streaming)

**session.rs** -- SessionManager for session lifecycle:
- `pub struct SessionManager` -- minimal wrapper for session ID tracking and turn counting
- `pub fn new(session: ChatSession) -> Self`
- `pub fn session(&self) -> &ChatSession`
- `pub fn increment_turn(&mut self)` -- tracks turn count for periodic memory extraction
- `pub fn should_extract_memory(&self) -> bool` -- per locked decision: "periodic during session (every N messages)". Return true every 10 turns.
- `pub fn mark_completed(&mut self)` -- set status to Completed, set ended_at

**service.rs** -- ChatService (orchestrates sessions):
- Follow the generic service pattern from BotService<B, S, F, H>:
  `pub struct ChatService<C: ChatRepository, M: MemoryRepository>`
- Fields: chat_repo (C), memory_repo (M)
- `impl<C: ChatRepository, M: MemoryRepository> ChatService<C, M>`:
  - `pub fn new(chat_repo: C, memory_repo: M) -> Self`
  - `pub async fn create_session(&self, bot_id: Uuid, model: &str) -> Result<ChatSession, RepositoryError>`
    - Create ChatSession with UUIDv7 id, started_at = now, status Active
    - Persist via chat_repo.create_session()
  - `pub async fn save_user_message(&self, session_id: Uuid, content: &str) -> Result<ChatMessage, RepositoryError>`
    - Create ChatMessage with UUIDv7 id, role User, created_at now
    - Persist via chat_repo.save_message()
  - `pub async fn save_assistant_message(&self, session_id: Uuid, content: &str, input_tokens: Option<u32>, output_tokens: Option<u32>, model: Option<String>, stop_reason: Option<String>, response_ms: Option<u64>) -> Result<ChatMessage, RepositoryError>`
    - Create ChatMessage with UUIDv7 id, role Assistant, all metadata
    - Persist via chat_repo.save_message()
  - `pub async fn update_session_title(&self, session_id: &Uuid, title: &str) -> Result<(), RepositoryError>`
  - `pub async fn end_session(&self, session_id: &Uuid) -> Result<(), RepositoryError>`
    - Update session: status Completed, ended_at = now
  - `pub async fn get_session(&self, session_id: &Uuid) -> Result<Option<ChatSession>, RepositoryError>`
  - `pub async fn list_sessions(&self, bot_id: &Uuid, limit: Option<i64>, offset: Option<i64>) -> Result<Vec<ChatSession>, RepositoryError>`
  - `pub async fn get_messages(&self, session_id: &Uuid, limit: Option<i64>, offset: Option<i64>) -> Result<Vec<ChatMessage>, RepositoryError>`
  - `pub async fn load_memories(&self, bot_id: &Uuid) -> Result<Vec<MemoryEntry>, RepositoryError>`
    - Load all memories for the bot (per locked decision: "All memories loaded into system prompt at session start")

**chat/mod.rs:** `pub mod repository; pub mod service; pub mod session;`
  </action>
  <verify>
Run `cargo check -p boternity-core` -- AgentEngine, SessionManager, and ChatService compile. Verify AgentEngine uses BoxLlmProvider (not a generic type parameter -- since providers are selected at runtime). Verify ChatService follows the generic trait pattern.
  </verify>
  <done>AgentEngine executes LLM calls with OTel instrumented spans, generates bot greetings, and returns streaming events. ChatService manages session lifecycle with immediate message persistence. SessionManager tracks turns for periodic memory extraction.</done>
</task>

</tasks>

<verification>
1. `cargo check -p boternity-core` passes
2. SystemPromptBuilder produces XML-tagged prompt with soul, identity, user, memories, instructions sections
3. AgentEngine.execute() returns a streaming event stream
4. AgentEngine.generate_greeting() produces a personality-driven greeting
5. ChatService is generic over ChatRepository + MemoryRepository traits
6. SessionManager tracks turns and triggers memory extraction every N messages
7. OTel spans are created for LLM calls with GenAI semantic convention attributes
</verification>

<success_criteria>
- System prompt includes SOUL.md, IDENTITY.md, USER.md, and memory entries with XML boundaries
- Agent engine streams LLM responses through BoxLlmProvider
- Chat service provides full session lifecycle management
- Messages are persisted immediately (not batched)
- OTel instrumentation on LLM calls
- `cargo check --workspace` passes
</success_criteria>

<output>
After completion, create `.planning/phases/02-single-agent-chat-llm/02-05-SUMMARY.md`
</output>
