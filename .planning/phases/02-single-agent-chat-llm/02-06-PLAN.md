---
phase: 02-single-agent-chat-llm
plan: 06
type: execute
wave: 3
depends_on: ["02-01", "02-03"]
files_modified:
  - crates/boternity-core/src/memory/extractor.rs
  - crates/boternity-core/src/memory/mod.rs
  - crates/boternity-core/src/agent/summarizer.rs
  - crates/boternity-core/src/agent/title.rs
  - crates/boternity-core/src/agent/mod.rs
autonomous: true

must_haves:
  truths:
    - "Memory extraction uses an LLM call to judge what's worth remembering from conversation messages"
    - "Extraction returns structured MemoryEntry objects with fact, category, and importance"
    - "Context summarizer condenses older messages into a summary when approaching token limit"
    - "Title generator creates a short descriptive title from the first exchange"
    - "Failed extractions are queued for retry (not silently dropped)"
  artifacts:
    - path: "crates/boternity-core/src/memory/extractor.rs"
      provides: "SessionMemoryExtractor extracting key points via LLM"
      contains: "pub struct SessionMemoryExtractor"
    - path: "crates/boternity-core/src/agent/summarizer.rs"
      provides: "ContextSummarizer for sliding window management"
      contains: "pub struct ContextSummarizer"
    - path: "crates/boternity-core/src/agent/title.rs"
      provides: "TitleGenerator for auto-naming sessions"
      contains: "pub fn generate_title"
  key_links:
    - from: "crates/boternity-core/src/memory/extractor.rs"
      to: "crates/boternity-core/src/llm/box_provider.rs"
      via: "uses BoxLlmProvider for extraction LLM calls"
      pattern: "BoxLlmProvider"
    - from: "crates/boternity-core/src/memory/extractor.rs"
      to: "crates/boternity-types/src/memory.rs"
      via: "returns Vec<MemoryEntry>"
      pattern: "MemoryEntry"
    - from: "crates/boternity-core/src/agent/summarizer.rs"
      to: "crates/boternity-core/src/llm/box_provider.rs"
      via: "uses BoxLlmProvider for summarization LLM calls"
      pattern: "BoxLlmProvider"
---

<objective>
Implement memory extraction, context window summarization, and session title generation -- the three auxiliary LLM-powered functions.

Purpose: Memory extraction gives bots persistent knowledge across sessions (locked decision: "LLM judges what's worth remembering"). Context summarization prevents personality drift in long conversations by condensing older messages (locked decision: "Sliding window with LLM-generated summary"). Title generation names sessions automatically (locked decision: "Auto-generated from first exchange by the LLM").

Output: SessionMemoryExtractor, ContextSummarizer, and TitleGenerator in boternity-core, each using BoxLlmProvider for their internal LLM calls.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-single-agent-chat-llm/02-RESEARCH.md
@.planning/phases/02-single-agent-chat-llm/02-CONTEXT.md
@.planning/phases/02-single-agent-chat-llm/02-01-SUMMARY.md
@.planning/phases/02-single-agent-chat-llm/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SessionMemoryExtractor</name>
  <files>
    crates/boternity-core/src/memory/extractor.rs
    crates/boternity-core/src/memory/mod.rs
  </files>
  <action>
Create the memory extraction system that uses an LLM call to identify key points from conversation messages.

**extractor.rs:**

Define the extraction system prompt as a const, using the prompt from RESEARCH.md (Code Examples section: Memory Extraction Prompt). Key rules:
- Extract ONLY information worth remembering across sessions
- Single, self-contained sentences per fact
- No greetings, pleasantries, or trivial exchanges
- Include name, preferences, goals, decisions, corrections
- Return JSON array: [{"fact": "...", "category": "...", "importance": N}]
- If nothing worth extracting, return empty array []

`pub struct SessionMemoryExtractor` -- no stored state, methods take the provider as parameter.

Methods:
- `pub async fn extract(provider: &BoxLlmProvider, messages: &[Message], bot_id: Uuid, session_id: Uuid) -> Result<Vec<MemoryEntry>, LlmError>`
  - Build a CompletionRequest with:
    - system: EXTRACTION_SYSTEM_PROMPT
    - messages: format the conversation as a single user message: "Extract key memories from this conversation:\n\n{formatted_messages}"
    - Format each message as "{role}: {content}" separated by newlines
    - model: use the same model from the provider (or hardcode a reasonable default like "claude-sonnet-4-20250514")
    - max_tokens: 2048 (extraction responses are short)
    - temperature: 0.0 (deterministic extraction)
    - stream: false
  - Call provider.complete()
  - Parse the response content as JSON array `Vec<RawMemoryEntry>` where RawMemoryEntry = { fact: String, category: String, importance: u8 }
  - Convert each to MemoryEntry with:
    - id: Uuid::now_v7()
    - bot_id, session_id from params
    - category: parse string to MemoryCategory enum (with fallback to Fact if unknown)
    - importance: clamp to 1..=5
    - source_message_id: None (could be enhanced later)
    - created_at: Utc::now()
    - is_manual: false
  - If JSON parsing fails, log warning with tracing::warn! and return empty Vec (graceful degradation)
  - Instrument with tracing span: "extract_memory {model}"

- `pub async fn extract_from_messages(provider: &BoxLlmProvider, messages: &[ChatMessage], bot_id: Uuid, session_id: Uuid) -> Result<Vec<MemoryEntry>, LlmError>`
  - Convenience method that converts ChatMessage to Message format first, then calls extract()

Add OTel span with gen_ai.operation.name = "extract_memory" attribute.

Update `memory/mod.rs` to add `pub mod extractor;` alongside `pub mod store;`
  </action>
  <verify>
Run `cargo check -p boternity-core` -- SessionMemoryExtractor compiles. Verify it uses BoxLlmProvider (not a generic LlmProvider) for the LLM call.
  </verify>
  <done>SessionMemoryExtractor uses an LLM call with a structured extraction prompt to identify key facts, preferences, and decisions from conversation messages, returning typed MemoryEntry objects.</done>
</task>

<task type="auto">
  <name>Task 2: Implement ContextSummarizer and TitleGenerator</name>
  <files>
    crates/boternity-core/src/agent/summarizer.rs
    crates/boternity-core/src/agent/title.rs
    crates/boternity-core/src/agent/mod.rs
  </files>
  <action>
**summarizer.rs** -- ContextSummarizer for sliding window management:

Per locked decision: "Sliding window with LLM-generated summary -- when approaching context limit, older messages are summarized and kept as context; bot stays coherent without user noticing."

Define SUMMARY_PROMPT as const, from RESEARCH.md:
```
Summarize the following conversation segment concisely. Preserve:
1. Key decisions and conclusions
2. Important facts mentioned
3. The user's current goals and context
4. Any unresolved questions

Keep the summary under 500 words. Write in third person.

<conversation>
{conversation_segment}
</conversation>
```

`pub struct ContextSummarizer` -- stateless utility.

Methods:
- `pub async fn summarize(provider: &BoxLlmProvider, messages: &[Message], model: &str) -> Result<String, LlmError>`
  - Build CompletionRequest:
    - system: SUMMARY_PROMPT
    - messages: format conversation as a single user message
    - max_tokens: 1024
    - temperature: 0.0 (deterministic)
    - stream: false
  - Call provider.complete(), return the response content
  - Instrument with tracing span: "summarize_context {model}"

- `pub fn select_messages_to_summarize(messages: &[Message], keep_recent: usize) -> (&[Message], &[Message])`
  - Split: return (messages_to_summarize, messages_to_keep)
  - Keep the most recent `keep_recent` messages (default 10)
  - Summarize everything before that
  - If total messages <= keep_recent, return empty slice for summarize and full slice for keep

**title.rs** -- TitleGenerator for auto-naming sessions:

Per locked decision: "Auto-generated from first exchange by the LLM (like ChatGPT's conversation naming)"

Define TITLE_GENERATION_PROMPT as const, from RESEARCH.md:
```
Generate a short, descriptive title (3-7 words) for this conversation based on the first exchange. The title should capture the main topic or intent. Return ONLY the title text, nothing else.
```

Methods:
- `pub async fn generate_title(provider: &BoxLlmProvider, first_user_message: &str, first_assistant_message: &str, model: &str) -> Result<String, LlmError>`
  - Build CompletionRequest:
    - system: TITLE_GENERATION_PROMPT
    - messages: [user: first_user_message, assistant: first_assistant_message, user: "What would be a good title for this conversation?"]
    - max_tokens: 50 (titles are short)
    - temperature: 0.3 (slightly creative but consistent)
    - stream: false
  - Call provider.complete(), trim whitespace and quotes from result
  - Instrument with tracing span: "generate_title {model}"

Update `agent/mod.rs` to add `pub mod summarizer; pub mod title;` alongside existing entries.
  </action>
  <verify>
Run `cargo check -p boternity-core` -- ContextSummarizer and TitleGenerator compile. Verify both use BoxLlmProvider for LLM calls. Verify prompts match the research document templates.
  </verify>
  <done>ContextSummarizer condenses older messages for sliding window management. TitleGenerator creates short conversation titles from the first exchange. Both are stateless utilities using BoxLlmProvider.</done>
</task>

</tasks>

<verification>
1. `cargo check -p boternity-core` passes
2. SessionMemoryExtractor parses LLM JSON response into Vec<MemoryEntry>
3. ContextSummarizer produces conversation summaries preserving key context
4. TitleGenerator produces 3-7 word titles
5. All three use BoxLlmProvider for LLM calls (not generic type params)
6. JSON parse failures in memory extraction degrade gracefully (empty Vec, not panic)
7. OTel spans instrument all LLM calls with gen_ai.operation.name
</verification>

<success_criteria>
- Memory extraction prompt follows RESEARCH.md template
- Extraction handles JSON parse failures gracefully
- Context summarizer identifies messages to summarize vs keep
- Title generator trims whitespace/quotes from LLM output
- All LLM calls are instrumented with tracing spans
- `cargo check --workspace` passes
</success_criteria>

<output>
After completion, create `.planning/phases/02-single-agent-chat-llm/02-06-SUMMARY.md`
</output>
