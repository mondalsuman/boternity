---
phase: 02-single-agent-chat-llm
plan: 07
type: execute
wave: 4
depends_on: ["02-02", "02-05", "02-06"]
files_modified:
  - crates/boternity-api/src/cli/chat/mod.rs
  - crates/boternity-api/src/cli/chat/renderer.rs
  - crates/boternity-api/src/cli/chat/input.rs
  - crates/boternity-api/src/cli/chat/banner.rs
  - crates/boternity-api/src/cli/chat/commands.rs
  - crates/boternity-api/src/cli/chat/loop.rs
  - crates/boternity-api/src/cli/mod.rs
  - crates/boternity-api/src/state.rs
  - crates/boternity-api/src/main.rs
  - crates/boternity-api/Cargo.toml
autonomous: false

must_haves:
  truths:
    - "User can run `bnity chat <bot>` and see a welcome banner with bot emoji, name, description, model"
    - "Bot sends a personality-driven greeting message when session opens"
    - "User types a message and sees streaming character-by-character response from the LLM"
    - "Thinking spinner appears while waiting for first token"
    - "Bot responses show markdown formatting: bold, italic, code blocks with syntax highlighting"
    - "Stats footer appears after each bot response: tokens, response time, model"
    - "Slash commands work: /help, /clear, /exit, /new, /history, /remember"
    - "Ctrl+D exits the session"
    - "Messages are persisted to SQLite immediately"
    - "Session title is auto-generated after the first exchange"
    - "Memory extraction runs periodically and at session end"
  artifacts:
    - path: "crates/boternity-api/src/cli/chat/loop.rs"
      provides: "Main chat loop: read input -> send to agent -> stream response -> persist"
      contains: "pub async fn run_chat_loop"
    - path: "crates/boternity-api/src/cli/chat/renderer.rs"
      provides: "Terminal markdown renderer with termimad + syntect"
      contains: "pub struct ChatRenderer"
    - path: "crates/boternity-api/src/cli/chat/input.rs"
      provides: "Async readline with multiline support"
      contains: "rustyline_async"
    - path: "crates/boternity-api/src/cli/chat/banner.rs"
      provides: "Welcome banner display"
      contains: "pub fn print_welcome_banner"
    - path: "crates/boternity-api/src/cli/chat/commands.rs"
      provides: "Slash command parsing and execution"
      contains: "ChatCommand"
  key_links:
    - from: "crates/boternity-api/src/cli/chat/loop.rs"
      to: "crates/boternity-core/src/agent/engine.rs"
      via: "calls AgentEngine.execute() for streaming LLM responses"
      pattern: "agent_engine|execute"
    - from: "crates/boternity-api/src/cli/chat/loop.rs"
      to: "crates/boternity-core/src/chat/service.rs"
      via: "calls ChatService for session/message persistence"
      pattern: "chat_service|save_"
    - from: "crates/boternity-api/src/cli/chat/renderer.rs"
      to: "termimad"
      via: "markdown rendering in terminal"
      pattern: "termimad::MadSkin|termimad"
    - from: "crates/boternity-api/src/main.rs"
      to: "crates/boternity-api/src/cli/chat/mod.rs"
      via: "Commands::Chat dispatches to chat module"
      pattern: "Commands::Chat"
---

<objective>
Build the interactive CLI chat experience with streaming, markdown rendering, and full session management.

Purpose: This is the user-facing feature that brings everything together. The user types `bnity chat <bot>` and enters a rich, interactive conversation with a personality-driven bot. Responses stream character-by-character, markdown is rendered with syntax-highlighted code blocks, and the bot's identity is visually distinct. This plan implements every locked "Streaming chat feel" decision.

Output: Complete CLI chat module in boternity-api with streaming display, markdown rendering, input handling, slash commands, welcome banner, thinking spinner, and wiring to agent engine + chat service.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-single-agent-chat-llm/02-RESEARCH.md
@.planning/phases/02-single-agent-chat-llm/02-CONTEXT.md
@.planning/phases/02-single-agent-chat-llm/02-05-SUMMARY.md
@.planning/phases/02-single-agent-chat-llm/02-06-SUMMARY.md
@crates/boternity-api/src/main.rs
@crates/boternity-api/src/state.rs
@crates/boternity-api/src/cli/mod.rs
@crates/boternity-api/src/cli/bot.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat UI components (renderer, input, banner, commands)</name>
  <files>
    crates/boternity-api/src/cli/chat/mod.rs
    crates/boternity-api/src/cli/chat/renderer.rs
    crates/boternity-api/src/cli/chat/input.rs
    crates/boternity-api/src/cli/chat/banner.rs
    crates/boternity-api/src/cli/chat/commands.rs
    crates/boternity-api/Cargo.toml
  </files>
  <action>
Add Phase 2 deps to boternity-api/Cargo.toml:
```toml
termimad = { workspace = true }
syntect = { workspace = true }
rustyline-async = { workspace = true }
crossterm = { workspace = true }
futures-util = { workspace = true }
boternity-observe = { workspace = true }
```

**renderer.rs** -- Terminal markdown rendering:
- `pub struct ChatRenderer` with fields: skin (termimad::MadSkin), syntax_set (syntect::parsing::SyntaxSet), theme_set (syntect::highlighting::ThemeSet)
- `pub fn new(accent_color: Option<crossterm::style::Color>) -> Self`
  - Create custom MadSkin, customize bold/italic/header colors if accent_color provided
  - Load syntect defaults: SyntaxSet::load_defaults_newlines(), ThemeSet::load_defaults()
- `pub fn render_final(&self, markdown: &str) -> String`
  - Split markdown into code blocks and non-code sections using a simple state machine (track ``` fences)
  - Code blocks: identify language from fence tag, highlight with syntect using "base16-ocean.dark" theme (or similar), output ANSI-colored text
  - Non-code sections: render with termimad skin
  - Combine and return the full styled string
- `pub fn print_streaming_token(&self, token: &str)`
  - Print token directly to stdout without markdown processing (raw typewriter during streaming)
  - Use `print!("{}", token); std::io::Write::flush(&mut std::io::stdout());`
- `pub fn print_stats_footer(&self, tokens: u32, response_ms: u64, model: &str)`
  - Per locked decision: "After every bot response show: tokens used, response time, model -- always visible"
  - Format: "  | {tokens} tokens . {time}s . {model}" using console dim styling
  - Time format: response_ms / 1000 with 1 decimal place

**input.rs** -- Async input handling:
- Use `rustyline_async::Readline` for async line editing
- `pub struct ChatInput` wrapping Readline
- `pub async fn new() -> Result<Self, ...>` -- create Readline with SharedWriter for concurrent output
- `pub async fn read_line(&mut self, prompt: &str) -> Result<InputEvent, ...>`
  - Map readline results to InputEvent enum: Message(String), Eof, Interrupted
- `pub fn shared_writer(&self) -> SharedWriter` -- for streaming output while readline is active
  - Per Pitfall 6: "Use rustyline-async's built-in mechanism for concurrent reading and writing"
- InputEvent enum: Message(String), Eof, Interrupted

Per locked decision on multiline: "Shift+Enter for newlines AND paste-aware (pasted multiline stays as-is until sent)". Note: rustyline-async may not support Shift+Enter natively. Implement best-effort: check if rustyline-async supports multiline mode. If not, document a TODO and accept single-line input for now with a note about paste detection in a future enhancement.

**banner.rs** -- Welcome banner:
Per locked decision: "Full banner on session start -- bot emoji + name + description + model + session info + hint about /help"
- `pub fn print_welcome_banner(name: &str, emoji: Option<&str>, description: &str, model: &str, session_id: &str)`
  - Print styled banner:
    ```
    ╭─────────────────────────────────────────╮
    │  {emoji} {name}                         │
    │  {description}                          │
    │                                         │
    │  Model: {model}                         │
    │  Session: {short_session_id}            │
    │                                         │
    │  Type /help for commands                │
    ╰─────────────────────────────────────────╯
    ```
  - Use console crate for colors: name in cyan bold, description in dim, model in yellow

**commands.rs** -- Slash command parsing:
Per locked decision: "Slash commands (/help, /clear, /exit, /new, /history) for discoverability + keyboard shortcuts"
- `pub enum ChatCommand` variants: Help, Clear, Exit, New, History, Remember(String), Unknown(String)
- `pub fn parse(input: &str) -> Option<ChatCommand>`
  - Match on input.trim(): "/help" -> Help, "/clear" -> Clear, "/exit" -> Exit, "/new" -> New, "/history" -> History, "/remember ..." -> Remember(rest), other "/" -> Unknown
  - Return None if input doesn't start with "/"
- `pub fn print_help()`
  - Print styled help listing all commands and their descriptions
  - Include keyboard shortcuts: Ctrl+D (exit), Ctrl+L (clear screen)

**chat/mod.rs:** `pub mod renderer; pub mod input; pub mod banner; pub mod commands; pub mod loop_runner;`
  </action>
  <verify>
Run `cargo check -p boternity-api` -- all chat UI components compile. Verify termimad and syntect are used in renderer.rs. Verify rustyline-async is used in input.rs.
  </verify>
  <done>Chat UI components exist: ChatRenderer with markdown + syntax highlighting, ChatInput with async readline, welcome banner, and slash command parsing. All locked streaming chat feel decisions are implemented.</done>
</task>

<task type="auto">
  <name>Task 2: Create main chat loop and wire into CLI</name>
  <files>
    crates/boternity-api/src/cli/chat/loop.rs
    crates/boternity-api/src/cli/mod.rs
    crates/boternity-api/src/state.rs
    crates/boternity-api/src/main.rs
  </files>
  <action>
**loop.rs** (named loop_runner.rs to avoid Rust keyword conflict, or use `r#loop` -- prefer loop_runner.rs):

Actually, name the file `loop_runner.rs` and the module `loop_runner` to avoid the `loop` keyword issue.

`pub async fn run_chat_loop(state: &AppState, bot_slug: &str, resume_session_id: Option<String>) -> anyhow::Result<()>`

This is the main orchestration function. It wires together ALL Phase 2 components:

1. **Bot resolution:** Look up bot by slug using state.bot_service. If not found, print error and return.

2. **API key resolution:** Get ANTHROPIC_API_KEY from state.secret_service. If missing, print error message telling user to run `bnity set secret ANTHROPIC_API_KEY`.

3. **Provider creation:** Create AnthropicProvider with the API key and bot's model (read from IDENTITY.md config). Wrap in BoxLlmProvider.

4. **Agent setup:**
   - Read SOUL.md, IDENTITY.md, USER.md from disk (using tokio::fs, per Pitfall 2)
   - Load memories from memory_repo for this bot
   - Process any pending memory extractions from previous sessions (retry queue)
   - Create AgentConfig from bot data
   - Create TokenBudget from provider capabilities
   - Create AgentContext with soul, identity, user, memories
   - Build system prompt via SystemPromptBuilder
   - Create AgentEngine with BoxLlmProvider

5. **Session creation:**
   - If resume_session_id: load existing session and messages, populate AgentContext conversation_history
   - Else: create new session via ChatService

6. **Welcome banner:** Print via banner::print_welcome_banner()

7. **Bot greeting:** Per locked decision: "Bot speaks first"
   - Call agent_engine.generate_greeting()
   - Print greeting with bot styling (emoji + colored name prefix)
   - Save as assistant message

8. **Chat loop:**
   - Create ChatInput
   - Create ChatRenderer
   - Create thinking spinner (indicatif::ProgressBar::new_spinner() with steady_tick)
   - Loop:
     a. Print user prompt: "{styled 'You >'}" per locked decision
     b. Read input from ChatInput
     c. If Eof or Interrupted: break (per locked decision: "Ctrl+D exits")
     d. Check for slash command: if starts with "/", parse and execute command, continue
     e. Save user message to DB immediately
     f. Add user message to AgentContext
     g. Start spinner with "thinking..."
     h. Call agent_engine.execute() to get stream
     i. Print bot name prefix (emoji + colored name)
     j. Consume stream: for each StreamEvent:
        - Connected: stop spinner
        - TextDelta: call renderer.print_streaming_token()
        - Usage: accumulate token counts
        - MessageDelta: record stop_reason
        - Done: break stream loop
     k. Print newline after streaming completes
     l. Re-render the complete response with markdown formatting (renderer.render_final())
        - Strategy: After streaming raw text, move cursor up and overwrite with formatted version
        - Alternative (simpler): Just print stats footer without re-rendering (raw streaming looks good for prose). Use render_final only if the response contains code blocks or complex markdown. For Phase 2 MVP: print raw during stream, print formatted final after stream ends by clearing the raw output and reprinting. If this proves too complex with terminal cursor manipulation, just skip re-rendering and show raw stream + stats footer.
     m. Print stats footer: tokens, response time, model
     n. Save assistant message to DB with token counts and response_ms
     o. Add assistant message to AgentContext
     p. Check if should generate title (after first exchange): call title generator, update session
     q. Check if should extract memory (every N turns): call extractor, save memories
     r. Check if should summarize context (token budget): call summarizer, update context

9. **Session end:**
   - Run final memory extraction
   - Mark session as completed
   - Print farewell message

**Error handling per locked decision:** "Show error in chat, then offer choice: retry / switch model / abort"
- On LlmError during streaming: stop spinner, print error styled as chat message, prompt user with choices
- On other errors: log and continue (don't crash the chat loop)

**Wire into CLI (state.rs updates):**
- Add to AppState: chat_service, memory_repo, agent engine factory (or just the repos and let chat loop create the engine)
- Add ChatService<SqliteChatRepository, SqliteMemoryRepository> to AppState
- Create type alias: `pub type ConcreteChatService = ChatService<SqliteChatRepository, SqliteMemoryRepository>`
- Wire in AppState::init(): create SqliteChatRepository and SqliteMemoryRepository from db_pool, create ConcreteChatService

**Wire into main.rs:**
- Add new command variant to Commands enum: `Chat { slug: String, #[arg(long)] resume: Option<String> }`
- In match: `Commands::Chat { slug, resume } => cli::chat::loop_runner::run_chat_loop(&state, &slug, resume).await?`

**Wire into cli/mod.rs:** Add `pub mod chat;`

Initialize tracing: In main.rs, replace the current tracing_subscriber setup with `boternity_observe::tracing_setup::init_tracing(false)` (OTel disabled by default, can be enabled with --otel flag later).
  </action>
  <verify>
Run `cargo check -p boternity-api` -- the chat loop compiles. Run `cargo build --release -p boternity-api` to verify the binary builds. Test basic command parsing: `cargo run -p boternity-api -- chat --help` should show the chat subcommand.
  </verify>
  <done>The complete chat loop orchestrates bot resolution, LLM provider creation, agent engine execution, streaming display, session persistence, memory extraction, and title generation. `bnity chat <bot>` is wired as a CLI command.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Full interactive CLI chat experience: `bnity chat <bot>` with streaming responses, markdown rendering, thinking spinner, welcome banner, bot greeting, stats footer, slash commands, memory extraction, and session persistence.
  </what-built>
  <how-to-verify>
1. Ensure you have an Anthropic API key: `bnity set secret ANTHROPIC_API_KEY`
2. Create a test bot if needed: `bnity create bot "Test Bot" --description "A friendly test assistant"`
3. Start a chat session: `bnity chat test-bot`
4. Verify: Welcome banner appears with bot name, emoji, description, model
5. Verify: Bot sends a personality-driven greeting message
6. Verify: "You >" prompt appears in a distinct color
7. Type a message and verify: Thinking spinner appears, then streaming character-by-character response
8. Verify: Stats footer shows after response (tokens, time, model)
9. Type a message with code (e.g., "Write a hello world in Python") and verify code formatting
10. Test slash commands: /help, /clear, /exit
11. Exit with Ctrl+D, then check: `bnity sessions test-bot` (from Plan 08)
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
1. `cargo build -p boternity-api` succeeds
2. `bnity chat <bot>` starts an interactive session
3. Welcome banner displays with bot identity
4. Bot greeting message appears
5. Streaming tokens display character-by-character
6. Thinking spinner shows while waiting for first token
7. Stats footer shows after each response
8. Slash commands work (/help, /exit, /clear)
9. Messages persist to SQLite (verify with `sqlite3` or future session browser)
10. Memory extraction runs (check session_memories table)
11. Session title is auto-generated
</verification>

<success_criteria>
- User experiences streaming chat with personality-driven bot
- All "Streaming chat feel" locked decisions are implemented
- Bot greeting, welcome banner, stats footer, slash commands all work
- Messages persist immediately to SQLite
- Memory extraction runs periodically and at session end
- Session title is auto-generated after first exchange
- Error display follows locked decision (show error + offer choices)
</success_criteria>

<output>
After completion, create `.planning/phases/02-single-agent-chat-llm/02-07-SUMMARY.md`
</output>
