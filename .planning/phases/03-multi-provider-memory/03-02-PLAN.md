---
phase: 03-multi-provider-memory
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - Cargo.toml
  - crates/boternity-infra/Cargo.toml
  - crates/boternity-infra/src/llm/mod.rs
  - crates/boternity-infra/src/llm/openai_compat/mod.rs
  - crates/boternity-infra/src/llm/openai_compat/config.rs
  - crates/boternity-infra/src/llm/openai_compat/streaming.rs
  - crates/boternity-infra/src/lib.rs
autonomous: true

must_haves:
  truths:
    - "OpenAiCompatibleProvider implements LlmProvider trait with both complete() and stream()"
    - "Factory functions exist for OpenAI, Gemini, Mistral, and GLM 4.7 with correct base URLs"
    - "Streaming maps async-openai events to existing StreamEvent enum"
    - "Token usage is reported in streaming mode (stream_options.include_usage = true)"
  artifacts:
    - path: "crates/boternity-infra/src/llm/openai_compat/mod.rs"
      provides: "OpenAiCompatibleProvider struct with factory methods"
      contains: "OpenAiCompatibleProvider"
    - path: "crates/boternity-infra/src/llm/openai_compat/streaming.rs"
      provides: "OpenAI SSE stream to StreamEvent adapter"
      contains: "StreamEvent"
    - path: "crates/boternity-infra/src/llm/openai_compat/config.rs"
      provides: "OpenAiCompatConfig and per-provider defaults"
      contains: "OpenAiCompatConfig"
  key_links:
    - from: "crates/boternity-infra/src/llm/openai_compat/mod.rs"
      to: "crates/boternity-core/src/llm/provider.rs"
      via: "implements LlmProvider"
      pattern: "impl LlmProvider for OpenAiCompatibleProvider"
    - from: "crates/boternity-infra/src/llm/openai_compat/streaming.rs"
      to: "crates/boternity-types/src/llm.rs"
      via: "maps to StreamEvent"
      pattern: "StreamEvent::TextDelta"
---

<objective>
Implement the OpenAI-compatible LLM provider using async-openai.

Purpose: A single provider implementation serves OpenAI, Google Gemini, Mistral, and GLM 4.7 -- four providers from one codebase via configurable base URLs. This covers LLMP-03, LLMP-04, LLMP-05, LLMP-08.
Output: OpenAiCompatibleProvider with streaming adapter, factory functions for each provider, workspace dependency additions.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-multi-provider-memory/03-RESEARCH.md
@.planning/phases/03-multi-provider-memory/03-CONTEXT.md
@.planning/phases/03-multi-provider-memory/03-01-SUMMARY.md
@crates/boternity-types/src/llm.rs
@crates/boternity-core/src/llm/provider.rs
@crates/boternity-core/src/llm/box_provider.rs
@crates/boternity-infra/src/llm/mod.rs
@crates/boternity-infra/src/llm/anthropic/client.rs
@crates/boternity-infra/Cargo.toml
@Cargo.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add workspace dependencies and provider config</name>
  <files>
    Cargo.toml
    crates/boternity-infra/Cargo.toml
    crates/boternity-infra/src/llm/openai_compat/config.rs
  </files>
  <action>
Add to workspace `Cargo.toml` `[workspace.dependencies]`:
```toml
async-openai = "0.32"
```
Do NOT add lancedb/fastembed/text-splitter/arrow yet -- those go in Plan 03-04. Only add what this plan needs.

Add to `crates/boternity-infra/Cargo.toml` dependencies:
```toml
async-openai = { workspace = true }
```
(Keep all existing deps unchanged.)

Create `crates/boternity-infra/src/llm/openai_compat/config.rs`:

1. `OpenAiCompatConfig` struct:
   - `provider_name: String`
   - `base_url: String`
   - `api_key: String`
   - `model: String`
   - `capabilities: ProviderCapabilities`

2. Per-provider default factory functions per RESEARCH.md Pattern 1:
   - `openai_defaults(api_key, model)` -> base_url: "https://api.openai.com/v1", capabilities with streaming/tool_calling/vision true, max_context 128K, max_output 16K
   - `gemini_defaults(api_key, model)` -> base_url: "https://generativelanguage.googleapis.com/v1beta/openai", max_context 1M, max_output 64K
   - `mistral_defaults(api_key, model)` -> base_url: "https://api.mistral.ai/v1", max_context 128K, max_output 32K
   - `glm_defaults(api_key, model)` -> base_url: "https://api.z.ai/api/paas/v4", vision false, max_context 200K, max_output 128K
   - `claude_subscription_defaults(model)` -> base_url: "http://localhost:3456/v1", api_key "dummy-key", extended_thinking true, max_context 200K, max_output 128K

3. Hard-coded cost table function `default_cost_table() -> HashMap<String, ProviderCostInfo>` with costs from RESEARCH.md Provider Cost Estimates table. Include anthropic, openai, gemini, mistral, glm, bedrock entries.
  </action>
  <verify>`cargo check -p boternity-infra` compiles with new async-openai dep.</verify>
  <done>Workspace dependencies added. Provider config structs and cost table created.</done>
</task>

<task type="auto">
  <name>Task 2: Implement OpenAiCompatibleProvider with streaming</name>
  <files>
    crates/boternity-infra/src/llm/openai_compat/mod.rs
    crates/boternity-infra/src/llm/openai_compat/streaming.rs
    crates/boternity-infra/src/llm/mod.rs
  </files>
  <action>
Create `crates/boternity-infra/src/llm/openai_compat/mod.rs`:

1. `OpenAiCompatibleProvider` struct:
   - `client: Client<OpenAIConfig>` (from async-openai)
   - `provider_name: String`
   - `model: String`
   - `capabilities: ProviderCapabilities`

2. `OpenAiCompatibleProvider::new(config: OpenAiCompatConfig) -> Self`:
   - Build `OpenAIConfig::new().with_api_key(&config.api_key).with_api_base(&config.base_url)`
   - Create `Client::with_config(openai_config)`

3. Factory methods using config defaults:
   - `pub fn openai(api_key: &str, model: &str) -> Self`
   - `pub fn gemini(api_key: &str, model: &str) -> Self`
   - `pub fn mistral(api_key: &str, model: &str) -> Self`
   - `pub fn glm(api_key: &str, model: &str) -> Self`
   - `pub fn claude_subscription(model: &str) -> Self` -- include doc comment noting experimental + ToS violation

4. Implement `LlmProvider` for `OpenAiCompatibleProvider`:
   - `name()` -> &self.provider_name
   - `capabilities()` -> &self.capabilities
   - `complete()`:
     - Build `CreateChatCompletionRequestArgs` from CompletionRequest
     - Map messages: CompletionRequest.system -> system message, CompletionRequest.messages -> user/assistant messages
     - Set model, max_tokens (as max_completion_tokens), temperature
     - Call `self.client.chat().create(request).await`
     - Map response: extract content from choices[0].message.content, usage from response.usage
     - Map finish_reason to StopReason (Stop -> EndTurn, Length -> MaxTokens, ToolCalls -> ToolUse)
   - `stream()`:
     - Build same request but with streaming enabled
     - CRITICAL: Set `stream_options: ChatCompletionStreamOptions { include_usage: Some(true) }` per RESEARCH.md Pitfall 9
     - Call `self.client.chat().create_stream(request).await`
     - Use streaming adapter from streaming.rs to map to StreamEvent
     - Return Pin<Box<dyn Stream>>
   - `count_tokens()`:
     - Character-based estimation: total chars / 4 (per project pattern from 02-05)
     - Return TokenCount { input_tokens }

Create `crates/boternity-infra/src/llm/openai_compat/streaming.rs`:

1. `map_openai_stream(stream: ChatCompletionResponseStream) -> Pin<Box<dyn Stream<Item = Result<StreamEvent, LlmError>> + Send>>`:
   - Use `async_stream::stream!` or `futures_util::StreamExt::map`
   - Emit `StreamEvent::Connected` first
   - For each chunk:
     - If `chunk.choices[0].delta.content` is Some(text) -> emit `StreamEvent::TextDelta { index: 0, text }`
     - If `chunk.choices[0].delta.tool_calls` is Some -> accumulate tool call JSON fragments (same pattern as AnthropicProvider)
     - If `chunk.choices[0].finish_reason` is Some:
       - Stop -> emit `StreamEvent::MessageDelta { stop_reason: StopReason::EndTurn }`
       - Length -> `StopReason::MaxTokens`
       - ToolCalls -> `StopReason::ToolUse`
     - If `chunk.usage` is Some(usage) -> emit `StreamEvent::Usage(Usage { input_tokens, output_tokens, ... })`
   - Emit `StreamEvent::Done` at end
   - Handle errors: map async-openai errors to `LlmError::Stream` or `LlmError::Provider`

Update `crates/boternity-infra/src/llm/mod.rs`: add `pub mod openai_compat;`

IMPORTANT: Do NOT derive Debug on OpenAiCompatibleProvider (defense-in-depth for API key, same pattern as AnthropicProvider per decision 02-03).
  </action>
  <verify>`cargo check -p boternity-infra` compiles. `cargo test -p boternity-infra` passes (existing tests still green).</verify>
  <done>OpenAiCompatibleProvider implements LlmProvider. Factory functions create providers for OpenAI, Gemini, Mistral, GLM 4.7, and Claude subscription. Streaming maps to StreamEvent with token usage.</done>
</task>

</tasks>

<verification>
- `cargo check --workspace` compiles
- `cargo test -p boternity-infra` passes
- `OpenAiCompatibleProvider::openai("key", "gpt-4o")` constructs without panic
- Streaming adapter correctly maps Connected -> TextDelta -> MessageDelta -> Usage -> Done sequence
</verification>

<success_criteria>
OpenAiCompatibleProvider implements LlmProvider for four providers via configurable base URL. Streaming properly maps async-openai events to the existing StreamEvent enum. Token usage reporting enabled.
</success_criteria>

<output>
After completion, create `.planning/phases/03-multi-provider-memory/03-02-SUMMARY.md`
</output>
