---
phase: 03-multi-provider-memory
plan: 08
type: execute
wave: 4
depends_on: ["03-07"]
files_modified:
  - crates/boternity-core/src/agent/prompt.rs
  - crates/boternity-core/src/agent/engine.rs
  - crates/boternity-core/src/agent/context.rs
  - crates/boternity-core/src/chat/service.rs
  - crates/boternity-core/src/memory/extractor.rs
autonomous: true

must_haves:
  truths:
    - "Every user message triggers a vector memory search"
    - "Retrieved memories are injected into system prompt in a <long_term_memory> XML section"
    - "Memories blend naturally (no explicit citation visible to user)"
    - "Verbose mode shows which memories were injected"
    - "Extracted memories are embedded and stored in LanceDB alongside SQLite"
    - "Auto re-embed detects model mismatch on startup"
    - "Memory extraction produces category assignments via LLM"
  artifacts:
    - path: "crates/boternity-core/src/agent/prompt.rs"
      provides: "System prompt with <long_term_memory> section from vector search"
      contains: "long_term_memory"
    - path: "crates/boternity-core/src/agent/engine.rs"
      provides: "Agent engine searches vector memory before each response"
      contains: "vector_memory"
  key_links:
    - from: "crates/boternity-core/src/agent/engine.rs"
      to: "crates/boternity-core/src/memory/vector.rs"
      via: "calls VectorMemoryStore::search on each user message"
      pattern: "vector_memory.*search"
    - from: "crates/boternity-core/src/agent/prompt.rs"
      to: "crates/boternity-types/src/memory.rs"
      via: "formats RankedMemory entries for system prompt"
      pattern: "RankedMemory"
    - from: "crates/boternity-core/src/memory/extractor.rs"
      to: "crates/boternity-core/src/memory/embedder.rs"
      via: "embeds extracted memories for LanceDB storage"
      pattern: "embedder.*embed"
---

<objective>
Integrate vector memory recall into the agent engine and chat service.

Purpose: Bots can now semantically recall relevant information from past conversations on every user message (MEMO-02). Memories are silently injected into the system prompt. This is the core intelligence upgrade from keyword-based to semantic memory.
Output: Agent engine searches vector memory per message, system prompt builder includes long-term memory section, memory extractor stores embeddings, auto re-embed on model change.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-multi-provider-memory/03-RESEARCH.md
@.planning/phases/03-multi-provider-memory/03-CONTEXT.md
@.planning/phases/03-multi-provider-memory/03-07-SUMMARY.md
@crates/boternity-core/src/agent/engine.rs
@crates/boternity-core/src/agent/prompt.rs
@crates/boternity-core/src/agent/context.rs
@crates/boternity-core/src/chat/service.rs
@crates/boternity-core/src/memory/extractor.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add vector memory search to agent engine and system prompt</name>
  <files>
    crates/boternity-core/src/agent/prompt.rs
    crates/boternity-core/src/agent/engine.rs
    crates/boternity-core/src/agent/context.rs
  </files>
  <action>
Update `agent/context.rs`:
1. Add to `AgentContext`:
   - `pub recalled_memories: Vec<RankedMemory>` -- memories retrieved for this turn
   - `pub verbose: bool` -- whether to show memory injection details (per user decision: `--verbose` flag)

Update `agent/prompt.rs` -- `SystemPromptBuilder`:
1. Add new section `<long_term_memory>` between existing `<session_memory>` and `<instructions>`:
   - Per user decision: "Memory search results silently injected into system prompt"
   - Format each RankedMemory as a natural-language fact: just the fact text, one per line
   - Include provenance for shared memories: "({provenance})" after the fact
   - Do NOT include similarity scores or metadata in the prompt -- blend naturally per user decision
   - Only include this section if recalled_memories is non-empty
   - Example output:
     ```
     <long_term_memory>
     The user prefers dark mode for all applications.
     The user works at a startup in the fintech space.
     Python is the user's primary programming language. (Written by ResearchBot)
     </long_term_memory>
     ```

2. Token budget: Recalled memories use the existing `memory` budget allocation (10% of context). If memories exceed the budget, truncate from lowest-relevance end.

Update `agent/engine.rs`:
1. Add generic parameter or dependency injection for vector memory:
   - AgentEngine already takes BoxLlmProvider. Add an optional vector memory store reference.
   - Option A (simpler): Add `vector_memory: Option<Arc<dyn VectorMemoryStoreDyn>>` to engine
   - Since VectorMemoryStore uses RPITIT, create a BoxVectorMemoryStore following the same pattern as BoxLlmProvider (VectorMemoryStoreDyn trait with boxed futures, blanket impl, BoxVectorMemoryStore wrapper).
   - Store as `Option<BoxVectorMemoryStore>` on AgentEngine

2. In `execute()` (or `build_request()`) before building the system prompt:
   - If vector_memory is Some:
     - Get the user's latest message text
     - Embed the user message via embedder: this requires an `Arc<dyn EmbedderDyn>` or similar -- pass embedder alongside vector_memory
     - Actually, for cleaner architecture: accept pre-computed `recalled_memories: Vec<RankedMemory>` rather than doing the search inside the engine. The caller (ChatService or CLI) does the search and passes results. This avoids making AgentEngine depend on Embedder.
     - Set `agent_context.recalled_memories = recalled_memories`
   - SystemPromptBuilder uses agent_context.recalled_memories

3. Verbose mode logging:
   - If `agent_context.verbose`:
     - Log to tracing::debug the number of memories recalled
     - The CLI layer will read this and print to stderr per user decision
   - This is a flag, not a print statement -- the CLI handles display
  </action>
  <verify>`cargo check -p boternity-core` compiles. SystemPromptBuilder generates correct XML with <long_term_memory> section.</verify>
  <done>Agent engine accepts recalled memories. System prompt includes <long_term_memory> section with natural-language facts. Verbose flag available for memory injection visibility.</done>
</task>

<task type="auto">
  <name>Task 2: Update memory extraction to embed and store in vector DB</name>
  <files>
    crates/boternity-core/src/chat/service.rs
    crates/boternity-core/src/memory/extractor.rs
  </files>
  <action>
Update `memory/extractor.rs` -- `SessionMemoryExtractor`:
1. The existing extractor produces MemoryEntry objects (fact, category, importance) and saves to SQLite via MemoryRepository.
2. Extend extraction to ALSO produce VectorMemoryEntry and store embeddings:
   - After extracting memories, accept a callback or trait object to embed and store in vector DB
   - Add `embed_and_store` step: for each extracted MemoryEntry:
     a. Create corresponding VectorMemoryEntry (link via source_memory_id = memory.id)
     b. The caller provides an embedder and vector store to complete the operation
   - Use semantic dedup: before adding, call `vector_store.check_duplicate()`. If near-duplicate exists, merge (update access_count of existing entry) instead of adding new.

3. Per user decision: "Auto-categorized memories (LLM assigns category during extraction)":
   - The existing extraction prompt already returns category. Ensure the extraction JSON format includes category field.
   - Map string categories from LLM output to MemoryCategory enum. Fall back to MemoryCategory::Fact for unknown categories.

Update `chat/service.rs` -- `ChatService`:
1. Add method `pub async fn search_memories_for_message(&self, bot_id: &Uuid, message: &str, embedder: &dyn EmbedderDyn, vector_store: &dyn VectorMemoryStoreDyn, limit: usize, min_similarity: f32) -> Result<Vec<RankedMemory>, ...>`:
   - Embed the message text
   - Call vector_store.search(bot_id, embedding, limit, min_similarity)
   - Per user decision: "Retrieve up to 10 memories with relevance threshold"
   - Default limit: 10, default min_similarity: 0.4

2. Add method `pub async fn embed_and_store_memories(&self, memories: &[MemoryEntry], embedder: &dyn EmbedderDyn, vector_store: &dyn VectorMemoryStoreDyn, audit_log: &dyn ...) -> Result<(), ...>`:
   - For each memory, embed the fact text
   - Check for duplicates
   - Store in vector DB
   - Log to audit trail (AuditAction::Add)

3. Auto re-embed on model change:
   - Add method `pub async fn check_and_reembed(&self, bot_id: &Uuid, embedder: &dyn EmbedderDyn, vector_store: &dyn VectorMemoryStoreDyn) -> Result<u64, ...>`:
     - Call `vector_store.get_all_for_reembedding(bot_id, embedder.model_name())`
     - If any entries found: re-embed each, update via `update_embedding()`
     - Per user decision (discretion): "Synchronous on startup if model changed"
     - Return count of re-embedded entries

Note: Use trait objects (dyn EmbedderDyn, dyn VectorMemoryStoreDyn) for these methods since they're called with concrete types from the API layer. Create DynEmbedder wrapper following same pattern as BoxLlmProvider if needed for object safety.
  </action>
  <verify>`cargo check -p boternity-core` compiles. Memory extraction flow includes embedding step.</verify>
  <done>Memory extraction embeds and stores in vector DB alongside SQLite. Semantic dedup prevents near-duplicate storage. Auto re-embed detects model mismatch. Chat service provides search_memories_for_message for the agent engine.</done>
</task>

</tasks>

<verification>
- `cargo check --workspace` compiles
- `cargo test -p boternity-core` passes
- System prompt includes <long_term_memory> section when memories are available
- Memory extraction creates both SQLite entries and LanceDB embeddings
- Semantic dedup prevents storing near-identical memories
</verification>

<success_criteria>
Every user message triggers vector memory search. Retrieved memories are injected into system prompt as natural-language facts. Memory extraction stores embeddings in LanceDB. Auto re-embed handles model changes. Verbose mode exposes memory injection.
</success_criteria>

<output>
After completion, create `.planning/phases/03-multi-provider-memory/03-08-SUMMARY.md`
</output>
