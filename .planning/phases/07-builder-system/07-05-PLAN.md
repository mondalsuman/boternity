---
phase: 07-builder-system
plan: 05
type: execute
wave: 3
depends_on: ["07-02", "07-03"]
files_modified:
  - crates/boternity-infra/src/builder/llm_builder.rs
  - crates/boternity-infra/src/builder/mod.rs
  - crates/boternity-infra/src/llm/anthropic/mod.rs
  - crates/boternity-infra/src/llm/bedrock/mod.rs
autonomous: true

must_haves:
  truths:
    - "LlmBuilderAgent calls Claude with structured output and parses BuilderTurn responses"
    - "output_config is wired through Anthropic and Bedrock providers"
    - "Each turn receives accumulated state context via Forge system prompt"
    - "Builder uses non-streaming calls (not streaming) for structured output"
  artifacts:
    - path: "crates/boternity-infra/src/builder/llm_builder.rs"
      provides: "LlmBuilderAgent implementing BuilderAgent"
      contains: "impl BuilderAgent for LlmBuilderAgent"
  key_links:
    - from: "crates/boternity-infra/src/builder/llm_builder.rs"
      to: "crates/boternity-core/src/builder/agent.rs"
      via: "implements BuilderAgent trait"
      pattern: "impl BuilderAgent"
    - from: "crates/boternity-infra/src/builder/llm_builder.rs"
      to: "crates/boternity-core/src/agent/engine.rs"
      via: "uses AgentEngine::execute_non_streaming for structured output"
      pattern: "execute_non_streaming"
---

<objective>
Implement the LLM-powered builder agent and wire output_config through providers.

Purpose: This is the brain of the builder system -- it calls Claude with structured output schema to get parseable BuilderTurn responses on each turn. Also wires the output_config field through Anthropic and Bedrock providers so the API request includes the JSON schema constraint.
Output: LlmBuilderAgent in boternity-infra, output_config wired through both providers.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-builder-system/07-RESEARCH.md
@.planning/phases/07-builder-system/07-CONTEXT.md
@.planning/phases/07-builder-system/07-01-SUMMARY.md
@.planning/phases/07-builder-system/07-02-SUMMARY.md
@.planning/phases/07-builder-system/07-03-SUMMARY.md
@crates/boternity-types/src/builder.rs
@crates/boternity-types/src/llm.rs
@crates/boternity-core/src/builder/agent.rs
@crates/boternity-core/src/builder/prompt.rs
@crates/boternity-core/src/builder/state.rs
@crates/boternity-core/src/agent/engine.rs
@crates/boternity-infra/src/llm/anthropic/mod.rs
@crates/boternity-infra/src/llm/bedrock/mod.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire output_config through Anthropic and Bedrock providers</name>
  <files>
    crates/boternity-infra/src/llm/anthropic/mod.rs
    crates/boternity-infra/src/llm/bedrock/mod.rs
  </files>
  <action>
    Per research CORRECTION 2, CompletionRequest now has `output_config: Option<OutputConfig>`. This needs to be forwarded to the provider-specific request formats.

    1. **Anthropic provider** (`crates/boternity-infra/src/llm/anthropic/mod.rs`):
    Find the `to_anthropic_request()` or equivalent function that converts CompletionRequest to the Anthropic API JSON body. Add:
    - If `request.output_config` is Some, include it in the JSON body as `"output_config": { "format": { "type": "json_schema", "json_schema": { "name": "...", "schema": ..., "strict": ... } } }`.
    - Per research CORRECTION 3: No beta header needed for Opus 4.6, Sonnet 4.5, Haiku 4.5 -- structured output is GA.
    - When output_config is present, force `stream: false` (research pitfall 3: structured output with streaming is not supported for builder use case).

    2. **Bedrock provider** (`crates/boternity-infra/src/llm/bedrock/mod.rs`):
    Same approach -- find the request builder and forward output_config if present. Bedrock uses the same Messages API format. Per research pitfall 10: ensure output_config is forwarded.

    3. Both providers should only serialize the output_config field when it's Some (skip_serializing_if = "Option::is_none" is already on the type, but verify the provider serializers respect it).
  </action>
  <verify>
    `cargo check -p boternity-infra` compiles.
    Existing LLM provider tests still pass: `cargo test -p boternity-infra llm`.
    Add a test that verifies output_config appears in the serialized Anthropic request body when set.
  </verify>
  <done>output_config forwarded through both Anthropic and Bedrock providers. Existing tests unaffected (backward compatible).</done>
</task>

<task type="auto">
  <name>Task 2: LlmBuilderAgent implementation</name>
  <files>
    crates/boternity-infra/src/builder/llm_builder.rs
    crates/boternity-infra/src/builder/mod.rs
  </files>
  <action>
    Create `crates/boternity-infra/src/builder/llm_builder.rs`:

    **LlmBuilderAgent** struct:
    ```rust
    pub struct LlmBuilderAgent {
        // No fields -- BoxLlmProvider passed per-call (stateless utility pattern from 02-06)
    }
    ```

    Implement `BuilderAgent for LlmBuilderAgent`:

    **`start()`:**
    1. Create new BuilderState with session_id and initial_description
    2. Run classify_purpose on initial_description to seed purpose_category
    3. Build Forge system prompt with BuilderMode::NewBot
    4. Generate JSON schema from BuilderTurn via `schemars::schema_for!(BuilderTurn)`
    5. Post-process schema with `add_additional_properties_false()` (CRITICAL per research FINDING 1)
    6. Create CompletionRequest with:
       - system: Forge system prompt
       - messages: [User message: initial_description]
       - output_config: Some(OutputConfig with BuilderTurn schema)
       - stream: false
       - temperature: 0.7
       - max_tokens: 2048
    7. Call provider.complete() (non-streaming)
    8. Parse response.content as BuilderTurn via serde_json::from_str
    9. If parse fails, return BuilderError::ParseError with raw content for debugging
    10. Return the BuilderTurn

    **`next_turn(state, answer)`:**
    1. Handle BuilderAnswer::Back -> call state.go_back(), rebuild prompt, call LLM for new question at the restored phase
    2. For other answers: format answer as string, call state.record_exchange with the previous question and this answer
    3. Update state.config fields based on the answer (match on current phase)
    4. Rebuild Forge system prompt with updated state
    5. Create CompletionRequest with full context (system prompt has conversation_summary)
    6. Messages: just the latest user answer (conversation context is in system prompt)
    7. Call provider.complete(), parse BuilderTurn
    8. If BuilderTurn is AskQuestion with a different phase, call state.advance_phase
    9. Return BuilderTurn

    **`resume(state)`:**
    1. Rebuild Forge system prompt from saved state
    2. Add instruction: "The user is resuming a previous builder session. Their progress so far is in the accumulated context. Continue from where they left off."
    3. Call LLM with system prompt and a user message "I'd like to continue where I left off"
    4. Parse and return BuilderTurn

    **`reconfigure(state, current_config)`:**
    1. Populate state.config from current_config
    2. Build Forge system prompt with BuilderMode::ReconfigureBot
    3. Call LLM with config context
    4. Parse and return BuilderTurn (should be a Clarify asking what to adjust)

    IMPORTANT: The LlmBuilderAgent needs a BoxLlmProvider for each call. Add a method:
    ```rust
    impl LlmBuilderAgent {
        pub fn new() -> Self { Self {} }

        /// All BuilderAgent methods need a provider. This is the internal call method.
        async fn call_llm(
            &self,
            provider: &BoxLlmProvider,
            system_prompt: String,
            user_message: String,
        ) -> Result<BuilderTurn, BuilderError> {
            // Build schema, create request, call, parse
        }
    }
    ```

    Since BuilderAgent trait doesn't take provider as param (it's surface-agnostic), the LlmBuilderAgent needs the provider set at construction. Revise: store `provider: BoxLlmProvider` as a field. The CLI/web surfaces will create LlmBuilderAgent with the provider when they start a session.

    Update `crates/boternity-infra/src/builder/mod.rs` to add `pub mod llm_builder;`.
  </action>
  <verify>
    `cargo check -p boternity-infra` compiles.
    `cargo test --workspace` passes.
    The LlmBuilderAgent is testable with a mock provider -- add a test that:
    - Creates LlmBuilderAgent with a mock BoxLlmProvider that returns a valid BuilderTurn JSON
    - Calls start() with "I want a coding assistant"
    - Verifies the returned BuilderTurn is AskQuestion
    This requires a mock provider or a test helper. If mock infrastructure exists from Phase 2/3 tests, reuse it. Otherwise, create a minimal MockLlmProvider that returns a static CompletionResponse.
  </verify>
  <done>LlmBuilderAgent implements BuilderAgent using Claude structured output. JSON schema generated from BuilderTurn with additionalProperties: false post-processing. Non-streaming calls for builder turns.</done>
</task>

</tasks>

<verification>
- `cargo check --workspace` compiles
- `cargo test --workspace` passes
- output_config forwarded through both Anthropic and Bedrock providers
- LlmBuilderAgent calls LLM with structured output schema
- BuilderTurn responses parse correctly from JSON
</verification>

<success_criteria>
- Anthropic and Bedrock providers forward output_config when present
- LlmBuilderAgent::start() generates first question from initial description
- LlmBuilderAgent::next_turn() advances conversation with accumulated context
- Back navigation works (go_back + re-ask)
- Resume and reconfigure modes work
- Structured output schema includes additionalProperties: false
</success_criteria>

<output>
After completion, create `.planning/phases/07-builder-system/07-05-SUMMARY.md`
</output>
