---
phase: 08-workflows-pipelines
plan: 04
type: execute
wave: 3
depends_on: ["08-02", "08-03"]
files_modified:
  - crates/boternity-core/src/workflow/executor.rs
  - crates/boternity-core/src/workflow/step_runner.rs
  - crates/boternity-core/src/workflow/checkpoint.rs
  - crates/boternity-core/src/workflow/mod.rs
autonomous: true

must_haves:
  truths:
    - "Workflow executor runs steps in topological wave order with parallel execution within waves"
    - "Each step is checkpointed to SQLite before and after execution"
    - "Crashed workflows can resume from last completed step"
    - "Step-level and workflow-level timeouts are enforced"
    - "Approval gate steps pause the workflow until user approves"
  artifacts:
    - path: "crates/boternity-core/src/workflow/executor.rs"
      provides: "WorkflowExecutor trait + DagExecutor impl with wave-based execution"
      min_lines: 150
    - path: "crates/boternity-core/src/workflow/step_runner.rs"
      provides: "StepRunner with Agent, Skill, Code, Http, Conditional, Loop, Approval, SubWorkflow runners"
      min_lines: 200
    - path: "crates/boternity-core/src/workflow/checkpoint.rs"
      provides: "CheckpointManager for durable execution state"
      min_lines: 80
  key_links:
    - from: "crates/boternity-core/src/workflow/executor.rs"
      to: "crates/boternity-core/src/workflow/dag.rs"
      via: "build_execution_plan for wave computation"
      pattern: "build_execution_plan"
    - from: "crates/boternity-core/src/workflow/executor.rs"
      to: "crates/boternity-core/src/repository/workflow.rs"
      via: "WorkflowRepository for checkpoint persistence"
      pattern: "WorkflowRepository"
    - from: "crates/boternity-core/src/workflow/step_runner.rs"
      to: "crates/boternity-core/src/workflow/context.rs"
      via: "WorkflowContext for step input/output"
      pattern: "WorkflowContext"
---

<objective>
Build the workflow execution engine with durable checkpointing, step runners for all step types, and crash recovery.

Purpose: This is the runtime that executes workflow DAGs. Steps run in parallel waves, each step is checkpointed to SQLite, and crashed workflows resume from the last completed step.
Output: WorkflowExecutor, StepRunner, and CheckpointManager in boternity-core.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-workflows-pipelines/08-RESEARCH.md
@crates/boternity-core/src/workflow/mod.rs
@crates/boternity-core/src/workflow/dag.rs
@crates/boternity-core/src/workflow/context.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: WorkflowExecutor with wave-based parallel execution and durable checkpointing</name>
  <files>crates/boternity-core/src/workflow/executor.rs, crates/boternity-core/src/workflow/checkpoint.rs, crates/boternity-core/src/workflow/mod.rs</files>
  <action>
Create `checkpoint.rs` with `CheckpointManager`:
- Generic over `R: WorkflowRepository`
- `new(repo: R) -> Self`
- `checkpoint_step_start(&self, run_id: &Uuid, step: &StepDefinition, attempt: u32) -> Result<Uuid, WorkflowError>` -- creates WorkflowStepLog with status=Pending, returns step_log_id
- `checkpoint_step_running(&self, step_log_id: &Uuid) -> Result<(), WorkflowError>` -- updates to Running
- `checkpoint_step_complete(&self, step_log_id: &Uuid, output: &serde_json::Value) -> Result<(), WorkflowError>` -- updates to Completed with output
- `checkpoint_step_failed(&self, step_log_id: &Uuid, error: &str) -> Result<(), WorkflowError>` -- updates to Failed with error
- `checkpoint_step_skipped(&self, step_log_id: &Uuid) -> Result<(), WorkflowError>` -- updates to Skipped
- `checkpoint_run_status(&self, run_id: &Uuid, status: WorkflowRunStatus, context: &WorkflowContext, error: Option<&str>) -> Result<(), WorkflowError>` -- updates run status + serialized context
- `get_completed_steps(&self, run_id: &Uuid) -> Result<HashSet<String>, WorkflowError>` -- returns set of completed step_ids
- `restore_context(&self, run: &WorkflowRun) -> Result<WorkflowContext, WorkflowError>` -- rebuilds context from run.context JSON

Create `executor.rs` with `WorkflowExecutor` trait (RPITIT):
- `execute(&self, def: &WorkflowDefinition, trigger_type: &str, trigger_payload: Option<serde_json::Value>) -> impl Future<Output = Result<WorkflowRun, WorkflowError>>`
- `resume(&self, run_id: &Uuid) -> impl Future<Output = Result<WorkflowRun, WorkflowError>>`
- `cancel(&self, run_id: &Uuid) -> impl Future<Output = Result<(), WorkflowError>>`

Implement `DagExecutor<R: WorkflowRepository>`:
- Fields: checkpoint: CheckpointManager<R>, event_bus: EventBus, cancellation_tokens: DashMap<Uuid, CancellationToken>
- `new(repo: R, event_bus: EventBus) -> Self`

`execute()` implementation:
1. Create WorkflowRun with status=Pending, Uuid::now_v7() for run_id
2. Save run to repo (status=Running)
3. Build execution plan via build_execution_plan()
4. Create WorkflowContext with trigger_payload
5. For each wave:
   a. Filter out already-completed steps (for resume)
   b. Spawn parallel tokio tasks for each step in wave
   c. Each task: checkpoint_step_start -> run step via StepRunner -> checkpoint_step_complete/failed
   d. If any step fails and retry config exists, retry up to max_attempts
   e. If fail-fast (default): cancel remaining wave tasks on first failure
   f. Collect outputs, update context
   g. Checkpoint context to run
6. On completion: update run status to Completed
7. On failure: update run status to Failed with error
8. Publish WorkflowEvent::RunCompleted/RunFailed to event_bus

`resume()` implementation:
1. Load run from repo
2. If status != Running (crashed), mark as Running
3. Restore context via checkpoint_manager
4. Get completed step IDs
5. Re-run execute() logic, skipping completed steps

Handle **concurrency control**: if workflow has concurrency set, use a per-workflow semaphore (stored in DashMap<String, Arc<Semaphore>>). Acquire permit before starting run, release on completion. Instance-level concurrency (not step-level) per research pitfall 3.

Handle **timeouts**: tokio::time::timeout wrapping each step execution (step.timeout_secs, default 300s). Workflow-level timeout wraps the entire execute loop.

Handle **approval gates**: when StepConfig::Approval encountered, checkpoint step as WaitingApproval, update run to Paused, publish event, return. Resume on approval.

Update `mod.rs` to export: `pub mod executor; pub mod checkpoint;`
  </action>
  <verify>`cargo check -p boternity-core` compiles</verify>
  <done>WorkflowExecutor processes waves in parallel with durable checkpointing, timeouts, concurrency control, and approval gates</done>
</task>

<task type="auto">
  <name>Task 2: Step runners for all 8 step types</name>
  <files>crates/boternity-core/src/workflow/step_runner.rs</files>
  <action>
Create `step_runner.rs` with `StepRunner` struct:

The StepRunner is the dispatcher that routes each step to the appropriate handler. It holds references to the services it needs (trait objects/generics for testability).

`StepRunner` is generic over `R: WorkflowRepository`. Takes references to:
- event_bus: EventBus (for publishing step lifecycle events)
- data_dir: PathBuf (for filesystem paths)

Methods:
- `run_step(&self, step: &StepDefinition, context: &WorkflowContext, executor_deps: &ExecutorDeps) -> Result<serde_json::Value, WorkflowError>`

Where `ExecutorDeps` is a struct holding shared references needed during execution:
- `event_bus: &EventBus`
- `data_dir: &Path`

Step implementations (each a private async function):

1. **Agent step**: `run_agent_step(config: &AgentStepConfig, context: &WorkflowContext, deps: &ExecutorDeps) -> Result<Value>`:
   - Resolve template variables in prompt using context.resolve_template()
   - The actual LLM call will be wired in Plan 09 (AppState integration). For now, return a placeholder that accepts the config and validates inputs.
   - Structure: prepare the prompt with context, return the config as-is for later wiring.

2. **Skill step**: `run_skill_step(config: &SkillStepConfig, context: &WorkflowContext, deps: &ExecutorDeps) -> Result<Value>`:
   - Resolve template in input
   - Similar to agent: prepare input, actual skill execution wired later
   - Return structured result with resolved input

3. **Code step**: `run_code_step(config: &CodeStepConfig, context: &WorkflowContext) -> Result<Value>`:
   - For TypeScript: placeholder for rustyscript integration (deferred to SDK plan)
   - For Wasm: placeholder for wasmtime integration
   - Return a placeholder indicating code type and resolved source

4. **HTTP step**: `run_http_step(config: &HttpStepConfig, context: &WorkflowContext) -> Result<Value>`:
   - Resolve templates in url, headers, body
   - Execute HTTP request via reqwest
   - Return { status: u16, headers: Map, body: String }
   - Enforce step timeout
   - This is fully implemented (no external service dependency)

5. **Conditional step**: `run_conditional_step(config: &ConditionalStepConfig, context: &WorkflowContext) -> Result<Value>`:
   - Evaluate condition using JEXL (placeholder -- actual eval wired in Plan 05)
   - Return { branch: "then" | "else", steps: Vec<String> } indicating which branch to take
   - The executor uses this result to determine which steps in the wave to run

6. **Loop step**: `run_loop_step(config: &LoopStepConfig, context: &WorkflowContext) -> Result<Value>`:
   - Evaluate loop condition
   - Return { iterations: u32, body_steps: Vec<String> }
   - max_iterations default: 100

7. **Approval step**: `run_approval_step(config: &ApprovalStepConfig) -> Result<Value>`:
   - Returns WorkflowError::ApprovalRequired (special error that executor handles by pausing)
   - Not a real "runner" -- signals executor to pause

8. **SubWorkflow step**: `run_sub_workflow_step(config: &SubWorkflowStepConfig, context: &WorkflowContext, depth: u32) -> Result<Value>`:
   - Check depth < MAX_SUB_WORKFLOW_DEPTH (5)
   - Return error if exceeded
   - Actual recursive execution wired when executor is fully integrated

Constants: `MAX_SUB_WORKFLOW_DEPTH: u32 = 5`, `DEFAULT_STEP_TIMEOUT_SECS: u64 = 300`.

Include tests:
- HTTP step with mock server (use a simple assertion on resolved URL)
- Conditional step branch selection
- Loop iteration cap enforcement
- SubWorkflow depth cap enforcement
  </action>
  <verify>`cargo test -p boternity-core -- workflow::step_runner` passes</verify>
  <done>All 8 step types have runners with template resolution, timeout handling, and proper error propagation</done>
</task>

</tasks>

<verification>
- `cargo check --workspace` compiles
- Step runners handle all 8 step types
- HTTP step makes real HTTP requests with template resolution
- Conditional and Loop steps evaluate correctly
- SubWorkflow depth cap prevents infinite recursion
- Checkpoint manager creates proper audit trail in SQLite
</verification>

<success_criteria>
- WorkflowExecutor runs DAG waves in parallel
- Each step is checkpointed before and after execution
- Crashed workflows can resume from last completed step
- Timeouts enforced at step and workflow level
- Approval gates pause workflow execution
- Concurrency limits at instance level
</success_criteria>

<output>
After completion, create `.planning/phases/08-workflows-pipelines/08-04-SUMMARY.md`
</output>
