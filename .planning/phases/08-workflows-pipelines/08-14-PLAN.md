---
phase: 08-workflows-pipelines
plan: 14
type: execute
wave: 1
depends_on: []
files_modified:
  - crates/boternity-infra/src/workflow/execution_context.rs
  - crates/boternity-infra/src/workflow/mod.rs
  - crates/boternity-infra/Cargo.toml
  - crates/boternity-api/src/state.rs
  - crates/boternity-api/src/http/handlers/workflow.rs
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "User can trigger a workflow via REST API and it actually executes (transitions from Pending to Running to Completed)"
    - "Agent steps in a workflow send prompts to real LLM providers and return actual responses"
    - "Skill steps in a workflow invoke real WASM skills and return actual output"
    - "HTTP steps in a workflow make real HTTP requests via reqwest and return status/body"
  artifacts:
    - path: "crates/boternity-infra/src/workflow/execution_context.rs"
      provides: "Real StepExecutionContext implementation wiring Agent/Skill/HTTP to actual services"
      exports: ["LiveExecutionContext"]
    - path: "crates/boternity-api/src/state.rs"
      provides: "DagExecutor on AppState as workflow_executor field"
      contains: "workflow_executor"
    - path: "crates/boternity-api/src/http/handlers/workflow.rs"
      provides: "trigger_workflow spawns background execution task"
      contains: "tokio::spawn"
  key_links:
    - from: "crates/boternity-api/src/http/handlers/workflow.rs"
      to: "crates/boternity-core/src/workflow/executor.rs"
      via: "state.workflow_executor.execute() in spawned task"
      pattern: "workflow_executor.*execute"
    - from: "crates/boternity-infra/src/workflow/execution_context.rs"
      to: "StepExecutionContext trait"
      via: "impl StepExecutionContext for LiveExecutionContext"
      pattern: "impl StepExecutionContext"
    - from: "crates/boternity-api/src/state.rs"
      to: "DagExecutor"
      via: "DagExecutor::new() in AppState::init()"
      pattern: "DagExecutor::new"
---

<objective>
Wire DagExecutor to AppState and implement real StepExecutionContext so that manually triggered workflows actually execute steps with real Agent/Skill/HTTP results instead of placeholders.

Purpose: Close the critical execution gap -- DagExecutor exists (598 lines) but is orphaned. trigger_workflow() creates Pending runs that never execute. This plan connects the executor to AppState, implements a real StepExecutionContext in the infra layer, and makes trigger_workflow() spawn a background task that calls executor.execute().

Output: Working end-to-end workflow execution for manual triggers. Steps produce real results (LLM responses for Agent, WASM output for Skill, HTTP responses for Http).
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-workflows-pipelines/08-VERIFICATION.md
@.planning/phases/08-workflows-pipelines/08-04-SUMMARY.md
@.planning/phases/08-workflows-pipelines/08-09-SUMMARY.md
@.planning/phases/08-workflows-pipelines/08-13-SUMMARY.md

Key existing code to reference:
@crates/boternity-core/src/workflow/executor.rs - DagExecutor with execute/resume/cancel
@crates/boternity-core/src/workflow/step_runner.rs - StepExecutionContext trait, StepRunner, PlaceholderExecutionContext
@crates/boternity-api/src/state.rs - AppState with Phase 8 fields (workflow_repo, message_bus, etc.)
@crates/boternity-api/src/http/handlers/workflow.rs - trigger_workflow() that creates Pending runs
@crates/boternity-infra/src/workflow/mod.rs - Existing infra workflow module (webhook_handler, file_trigger)
</context>

<tasks>

<task type="auto">
  <name>Task 1: LiveExecutionContext in infra layer + DagExecutor on AppState</name>
  <files>
    crates/boternity-infra/src/workflow/execution_context.rs
    crates/boternity-infra/src/workflow/mod.rs
    crates/boternity-infra/Cargo.toml
    crates/boternity-api/src/state.rs
  </files>
  <action>
    Create `crates/boternity-infra/src/workflow/execution_context.rs` implementing `StepExecutionContext` from boternity-core with real service wiring:

    **LiveExecutionContext struct:**
    - Fields: `data_dir: PathBuf`, `db_pool: DatabasePool`, `secret_service: Arc<SecretService>`, `skill_store: Arc<SkillStore>`, `wasm_runtime: Arc<WasmRuntime>`
    - Implements `StepExecutionContext` trait (from boternity-core::workflow::step_runner)

    **execute_agent implementation:**
    1. Resolve the bot by slug from data_dir (read IDENTITY.md frontmatter for model name)
    2. Create a single BoxLlmProvider using the secret_service to get ANTHROPIC_API_KEY (same pattern as AppState::create_single_provider)
    3. Use the provider to send a non-streaming completion request with the prompt as user message
    4. Return JSON: `{"type": "agent", "bot": bot, "response": llm_response_text}`
    5. On error, return StepError::ExecutionFailed with the error message
    6. If bot slug can't be resolved, return an error rather than panicking

    **execute_skill implementation:**
    1. Look up the skill by slug from skill_store.get_skill(skill_slug)
    2. If skill is a Prompt type, return its body with the input substituted
    3. If skill is WASM type, use wasm_runtime to execute it with the input
    4. Return JSON: `{"type": "skill", "skill": skill_name, "output": result}`
    5. If skill not found, return StepError::ExecutionFailed

    **execute_http implementation:**
    1. Use reqwest::Client (add reqwest to boternity-infra Cargo.toml if not already present -- check first)
    2. Build request from method/url/headers/body
    3. Execute the request with a 30-second timeout
    4. Return JSON: `{"type": "http", "status": status_code, "body": response_body_text, "headers": response_headers}`
    5. On network error, return StepError::ExecutionFailed

    **IMPORTANT architecture notes:**
    - boternity-core must NOT depend on boternity-infra (clean architecture rule)
    - The StepExecutionContext trait is defined in boternity-core (step_runner.rs)
    - The LiveExecutionContext implementation lives in boternity-infra (this file)
    - This follows the same pattern as other infra implementations (SqliteBotRepository implements BotRepository trait from core)

    **Update `crates/boternity-infra/src/workflow/mod.rs`:**
    - Add `pub mod execution_context;` alongside existing `pub mod webhook_handler;` and `pub mod file_trigger;`

    **Update `crates/boternity-infra/Cargo.toml`:**
    - Add reqwest dependency if not already present (check workspace Cargo.toml first for existing reqwest dep)
    - reqwest needs `json` feature for JSON parsing

    **Update `crates/boternity-api/src/state.rs`:**
    1. Add `use boternity_core::workflow::executor::DagExecutor;` and `use boternity_core::workflow::step_runner::StepRunner;`
    2. Add `use boternity_infra::workflow::execution_context::LiveExecutionContext;`
    3. Add field to AppState struct: `pub workflow_executor: Arc<DagExecutor<SqliteWorkflowRepository>>`
    4. In AppState::init(), after the Phase 8 services block and crash recovery:
       - Create LiveExecutionContext with data_dir, db_pool, secret_service, skill_store, wasm_runtime
       - Create StepRunner::with_context(data_dir, Arc::new(live_ctx))
       - Create DagExecutor, but note: DagExecutor::new() takes (repo, event_bus, data_dir). The step_runner is created internally by DagExecutor::new(). We need to modify the approach:
         - Option A: Add a method to DagExecutor to replace its step_runner. Add `pub fn with_step_runner(mut self, runner: StepRunner) -> Self` method.
         - Option B: Change DagExecutor::new() to accept an optional StepExecutionContext.
         - Choose Option B: modify DagExecutor::new() signature to accept `exec_ctx: Option<Arc<dyn StepExecutionContext>>` and use it to create StepRunner. If None, uses PlaceholderExecutionContext. This keeps the API clean.
       - Actually, looking at executor.rs line 131: `step_runner: Arc::new(StepRunner::new(data_dir))` -- we should add `DagExecutor::with_context()` that takes an `Arc<dyn StepExecutionContext>` and creates StepRunner::with_context().
       - Add to DagExecutor in executor.rs: `pub fn with_execution_context(repo: R, event_bus: EventBus, data_dir: PathBuf, exec_ctx: Arc<dyn StepExecutionContext>) -> Self` that uses `StepRunner::with_context(data_dir, exec_ctx)`
       - In AppState::init(), use `DagExecutor::with_execution_context(...)` with the LiveExecutionContext
    5. Add workflow_executor to the Ok(Self { ... }) struct
    6. Note: DagExecutor needs a separate WorkflowRepository instance (it takes ownership via CheckpointManager). Create a second SqliteWorkflowRepository instance for the executor: `SqliteWorkflowRepository::new(db_pool.clone())`
  </action>
  <verify>
    `cargo check -p boternity-infra -p boternity-api` compiles cleanly.
    Grep for `workflow_executor` in state.rs to confirm field exists.
    Grep for `LiveExecutionContext` in execution_context.rs to confirm struct exists.
    Grep for `with_execution_context` in executor.rs to confirm new constructor exists.
  </verify>
  <done>
    AppState has a `workflow_executor: Arc<DagExecutor<SqliteWorkflowRepository>>` field initialized with a real LiveExecutionContext that wires Agent steps to LLM providers, Skill steps to WASM runtime, and HTTP steps to reqwest.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire trigger_workflow() and webhook handler to spawn executor</name>
  <files>
    crates/boternity-api/src/http/handlers/workflow.rs
    crates/boternity-api/src/http/handlers/webhook.rs
  </files>
  <action>
    **Update `trigger_workflow()` in workflow.rs:**
    1. After creating the run record (line ~238), instead of just returning the Pending run:
    2. Clone the executor and definition for the spawned task: `let executor = Arc::clone(&state.workflow_executor);` and `let def = def.clone();`
    3. Extract trigger_payload before the run creation (it's already available as `payload`)
    4. After `state.workflow_repo.create_run(&run).await?;`, spawn a background task:
       ```rust
       let trigger_payload = run.trigger_payload.clone();
       tokio::spawn(async move {
           match executor.execute(&def, "manual", trigger_payload).await {
               Ok(result) => {
                   tracing::info!(
                       run_id = %result.run_id,
                       status = ?result.status,
                       steps = result.completed_steps.len(),
                       "workflow execution completed"
                   );
               }
               Err(e) => {
                   tracing::error!(
                       error = %e,
                       "workflow execution failed"
                   );
               }
           }
       });
       ```
    5. Change the run status in the response from `Pending` to indicate it's been submitted for execution. Keep the immediate response returning the run_id -- the user can poll status via GET /runs/:run_id.
    6. IMPORTANT: The executor.execute() creates its OWN run record internally (see executor.rs line 424). But trigger_workflow() already created a Pending run. To avoid duplicate runs, there are two approaches:
       - Approach A: Remove the run creation from trigger_workflow() and let the executor handle it
       - Approach B: Remove the run creation from executor.execute() and have trigger_workflow() transition the existing run
       - Choose Approach A: Remove the Pending run creation from trigger_workflow(). The executor creates its own Running run. This is cleaner because the executor manages the full lifecycle. The trigger_workflow() response should return a JSON indicating the workflow was triggered (but the run_id isn't known until the executor creates it -- so return the workflow_id and a "triggered" status).
       - ACTUALLY, looking more carefully: the executor.execute() creates a new run with Uuid::now_v7() and status Running. If we let the endpoint NOT create a run, the user won't get a run_id in the response. Better approach:
       - Approach C: Keep the Pending run creation in trigger_workflow() for the immediate response, but pass the existing run_id to the executor. Modify executor to accept an optional pre-created run_id. This is too complex.
       - SIMPLEST approach: Remove run creation from trigger_workflow(). Spawn executor in background. Return the workflow_id and a "submitted" status. The executor creates the actual run which the user can find via GET /workflows/:id/runs. This mirrors async job submission patterns.

    **Update `receive_webhook()` in webhook.rs:**
    1. Same pattern: After verifying the webhook and loading the workflow definition, spawn executor in background instead of creating a Pending run
    2. Remove the Pending run creation (lines 85-104)
    3. Spawn: `let executor = Arc::clone(&state.workflow_executor);`
    4. Clone def and payload for the spawned task
    5. `tokio::spawn(async move { executor.execute(&def, "webhook", trigger_payload).await; });`
    6. Return response with workflow_id and "triggered" status

    **Update `approve_run()` in workflow.rs:**
    1. After transitioning status to Running, spawn executor.resume() in background:
       ```rust
       let executor = Arc::clone(&state.workflow_executor);
       let def = state.workflow_repo.get_definition(&run.workflow_id).await...;
       tokio::spawn(async move {
           let _ = executor.resume(run_id, &def).await;
       });
       ```
  </action>
  <verify>
    `cargo check -p boternity-api` compiles cleanly.
    Grep for `tokio::spawn` in workflow.rs to confirm background execution task exists.
    Grep for `executor.execute` in workflow.rs to confirm executor is called.
    Grep for `executor.execute` in webhook.rs to confirm webhook triggers execution.
    Grep for `executor.resume` in workflow.rs to confirm approve resumes execution.
  </verify>
  <done>
    trigger_workflow() spawns a background task that calls DagExecutor.execute(). receive_webhook() spawns a background task that calls DagExecutor.execute() with "webhook" trigger type. approve_run() spawns executor.resume() for paused workflows. Workflows transition from submission to Running to Completed/Failed.
  </done>
</task>

</tasks>

<verification>
1. `cargo check -p boternity-infra -p boternity-api` -- all code compiles
2. `cargo test -p boternity-core -- workflow` -- existing executor and step_runner tests still pass (PlaceholderExecutionContext is still the default for DagExecutor::new())
3. `cargo test -p boternity-infra -- workflow` -- existing webhook and file_trigger tests still pass
4. Grep `workflow_executor` in state.rs shows the field on AppState
5. Grep `LiveExecutionContext` in execution_context.rs shows the real implementation
6. Grep `tokio::spawn` in workflow.rs shows background execution
</verification>

<success_criteria>
- DagExecutor is instantiated on AppState with LiveExecutionContext providing real Agent/Skill/HTTP execution
- trigger_workflow() HTTP handler spawns a background task that runs the workflow via DagExecutor.execute()
- receive_webhook() HTTP handler spawns a background task that runs the workflow via DagExecutor.execute()
- approve_run() resumes paused workflows via DagExecutor.resume()
- Existing tests continue to pass (PlaceholderExecutionContext remains the default for tests)
</success_criteria>

<output>
After completion, create `.planning/phases/08-workflows-pipelines/08-14-SUMMARY.md`
</output>
