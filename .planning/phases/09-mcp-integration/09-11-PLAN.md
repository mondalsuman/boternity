---
phase: 09-mcp-integration
plan: 11
type: execute
wave: 5
depends_on: ["09-04", "09-06", "09-07"]
files_modified:
  - crates/boternity-api/src/state.rs
  - crates/boternity-api/src/cli/chat/mod.rs
  - crates/boternity-api/src/http/handlers/chat.rs
autonomous: true

must_haves:
  truths:
    - "AppState includes MCP client pool, config store, keystore, audit logger, and MCP repo"
    - "Chat handler wires MCP tools into agent conversation loop"
    - "MCP tool calls display as collapsible blocks in CLI"
    - "Health ping background task runs for connected MCP servers"
  artifacts:
    - path: "crates/boternity-api/src/state.rs"
      provides: "AppState with Phase 9 MCP services"
      contains: "mcp_client_pool"
    - path: "crates/boternity-api/src/cli/chat/mod.rs"
      provides: "CLI chat with MCP tool use rendering"
      contains: "mcp_tool"
  key_links:
    - from: "crates/boternity-api/src/state.rs"
      to: "crates/boternity-infra/src/mcp/client_manager.rs"
      via: "RmcpClientManager on AppState"
      pattern: "RmcpClientManager"
    - from: "crates/boternity-api/src/cli/chat/mod.rs"
      to: "crates/boternity-infra/src/mcp/agent_integration.rs"
      via: "prepare_mcp_context, handle_tool_use"
      pattern: "prepare_mcp_context"
---

<objective>
Wire MCP services into AppState and integrate MCP tool use into the chat handlers (CLI and HTTP).

Purpose: This is the integration layer that connects all MCP infrastructure into the running application. AppState gets MCP services, the chat handlers use MCP tools during conversation, and a background health ping task monitors connected servers.
Output: AppState Phase 9 fields, chat handler MCP integration, health ping background task.
</objective>

<execution_context>
@/Users/smxaz7/.claude/get-shit-done/workflows/execute-plan.md
@/Users/smxaz7/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-mcp-integration/09-RESEARCH.md
@crates/boternity-api/src/state.rs
@crates/boternity-api/src/cli/chat/mod.rs
@crates/boternity-api/src/http/handlers/chat.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: AppState Phase 9 services and health ping</name>
  <files>crates/boternity-api/src/state.rs</files>
  <action>
Add Phase 9 MCP services to AppState struct:

```rust
// --- Phase 9 services ---
/// MCP client connection pool for consuming external MCP servers.
pub mcp_client_pool: Arc<RmcpClientManager>,
/// JSON file-based MCP server configuration store.
pub mcp_config_store: Arc<JsonMcpConfigStore>,
/// Separate MCP credential keystore.
pub mcp_keystore: Arc<McpKeystore>,
/// SQLite-backed MCP connection and audit repository.
pub mcp_repo: Arc<SqliteMcpRepository>,
/// SQLite-backed MCP audit logger.
pub mcp_audit_logger: Arc<SqliteMcpAuditLog>,
/// Tool description and result sanitizer.
pub mcp_sanitizer: Arc<DefaultToolSanitizer>,
```

In `AppState::init()`:

```rust
// --- Phase 9 services ---
let mcp_repo = Arc::new(SqliteMcpRepository::new(db_pool.clone()));
let mcp_audit_logger = Arc::new(SqliteMcpAuditLog::new(db_pool.clone()));
let mcp_sanitizer = Arc::new(DefaultToolSanitizer::new());
let mcp_config_store = Arc::new(JsonMcpConfigStore::new(&data_dir));
let mcp_keystore = Arc::new(McpKeystore::new(&data_dir));
let mcp_client_pool = Arc::new(RmcpClientManager::new(
    mcp_sanitizer.clone(),
    mcp_audit_logger.clone(),
    mcp_repo.clone(),
));
```

Add all fields to the `Ok(Self { ... })` return.

**Health ping background task:**

After constructing AppState, start a background task for health pings:
```rust
// Start MCP health ping background task
let health_pool = mcp_client_pool.clone();
let health_audit = mcp_audit_logger.clone();
tokio::spawn(async move {
    health_ping_loop(health_pool, 60, health_audit).await;
});
```

The `health_ping_loop` function (in mcp/client_manager.rs or a separate module):
- Runs every 60 seconds (per Claude's discretion on interval)
- For each connected server, sends a ping
- Updates last_health_ping timestamp in SQLite
- On failure: logs warning, logs audit entry, attempts reconnection with exponential backoff (1s, 2s, 4s, max 60s)
- On repeated failure (3 consecutive): updates status to Error, stops retrying until user reconnects

Import all necessary types at the top of state.rs.
  </action>
  <verify>`cargo check -p boternity-api` compiles with Phase 9 AppState fields</verify>
  <done>AppState includes all MCP services, health ping background task runs on startup</done>
</task>

<task type="auto">
  <name>Task 2: Wire MCP tools into chat handlers</name>
  <files>crates/boternity-api/src/cli/chat/mod.rs, crates/boternity-api/src/http/handlers/chat.rs</files>
  <action>
Integrate MCP tool use into both CLI and HTTP chat handlers.

**CLI chat handler (cli/chat/mod.rs):**

Before the chat loop starts:
1. Load MCP tools: `let mcp_tools = prepare_mcp_context(&*mcp_client_pool, bot_id, &permissions).await;`
2. If tools non-empty, print `"  MCP tools: {n} tools from {m} servers"` in session header

In the chat loop (where agent response is processed):
1. Check if LLM response contains tool_use content blocks
2. For each tool_use block:
   a. Print collapsible tool call header: `"[Tool: {name}]"` with dimmed input JSON
   b. Call `handle_tool_use(client_manager, sanitizer, tool_name, input)`
   c. Print result (syntax-highlighted JSON if possible, truncated for display)
   d. Append tool result to conversation context
   e. Re-send to LLM with tool result
3. Continue until LLM gives a text response (no more tool_use blocks)

For CLI rendering of tool calls, use a format like:
```
  > Tool: read_file [filesystem]
    Input: {"path": "/etc/hostname"}
    Result: "my-hostname"
    (42ms)
```

Use console crate for dimming the tool call details. Keep it compact but informative.

**HTTP chat handler (http/handlers/chat.rs):**

Similar integration but for the SSE streaming endpoint:
1. Load MCP tools before starting the stream
2. Include tools in the CompletionRequest sent to the LLM
3. When tool_use is detected in the stream, pause streaming, call tool, append result, resume
4. Send tool call events through SSE so the web UI can render collapsible blocks:
   ```json
   {"type": "tool_use", "name": "read_file", "server": "filesystem", "input": {...}}
   {"type": "tool_result", "name": "read_file", "result": "...", "duration_ms": 42}
   ```

For graceful degradation: if MCP client pool has no connections, skip all MCP-related code paths. The conversation proceeds normally without tool access.

Note: This depends heavily on the existing agent engine's tool use mechanism. If the LLM provider (Anthropic) already returns tool_use blocks and the engine doesn't handle them, add the tool use loop. If there's already a tool use mechanism for skills, extend it to include MCP tools.
  </action>
  <verify>`cargo check -p boternity-api` compiles with MCP chat integration</verify>
  <done>Chat handlers wire MCP tools into conversation, render tool calls in CLI, send tool events via SSE</done>
</task>

</tasks>

<verification>
- `cargo check -p boternity-api` passes
- AppState has all Phase 9 fields
- Health ping task spawned on init
- CLI chat shows MCP tool calls when tools are available
- HTTP SSE stream includes tool_use/tool_result events
- No MCP servers = normal chat (graceful degradation)
</verification>

<success_criteria>
MCP services are wired into AppState and running. Chat handlers in both CLI and HTTP integrate MCP tools into the conversation loop with tool call rendering (collapsible blocks in CLI, SSE events for web UI). Health pings monitor connected servers. Graceful degradation when no MCP servers are connected.
</success_criteria>

<output>
After completion, create `.planning/phases/09-mcp-integration/09-11-SUMMARY.md`
</output>
